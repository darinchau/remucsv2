# Config file for autoencoder training

## Dataset parameters
separator: "14_SP-UVR-4B-44100-2.pth"             # Separator model to use for training.
nstems: 2                                         # Number of stems in the dataset. Has to be adjusted according to the dataset.
dataset_dir: "E:/audio"                           # Directory containing the dataset
output_dir: "E:/output"                           # Directory to save the trained models and checkpoints
val_count: 100                                    # Number of validation samples
sample_rate: 44100                                # Sample rate of the audio
length: 131072                                    # Length of each audio sample in samples (5s)

## Model parameters
codebook_size: 4096                               # Size of each codebook for VQ-VAE
nquantizers: 4                                    # Number of quantizers in the RVQ component
z_channels: 256                                   # Number of channels in the latent space
down_channels : [64, 128, 256, 256]               # Number of channels in each downsampling layer
mid_channels : [256, 256]                         # Number of channels in each middle layer
down_sample : [4, 8, 8]                           # Downsampling factors for each downsampling layer
attn_down : [False, False, False]                 # Whether to use attention in each downsampling layer
norm_channels: 32                                 # Number of channels for normalization
num_heads: 4                                      # Number of attention heads
num_down_layers : 2                               # Number of downsampling layers
num_mid_layers : 2                                # Number of middle layers
num_up_layers : 2                                 # Number of upsampling layers

## Training parameters
seed : 1943                                       # Random seed
gradient_checkpointing: True                      # Whether to use gradient checkpointing
num_workers_dl: 4                                 # Number of workers for data loading
batch_size: 4                                     # Batch size for autoencoder training
codebook_weight: 1                                # Weight of codebook loss
commitment_beta: 0.2                              # Weight of commitment loss
bands: [2048, 1024, 512, 256, 128, 64]            # Bands to use for the Multiband Spectral Loss
p_skip_quantization: 0.1                          # Probability of skipping quantization during the training step to ensure efficient gradient flow
steps: 99999                                      # Number of training steps to perform in total
autoencoder_lr: 0.00001                           # Learning rate for the autoencoder
autoencoder_acc_steps: 16                         # Number of accumulation steps for the autoencoder
save_steps: 2048                                  # Number of steps between saving images
loss: "l1"                                        # Loss function to use for training. Can be 'l1', 'smoothl1' or 'l2'
ckpt_name: 'vqvae_autoencoder_ckpt.pth'           # Checkpoint name for the VQ-VAE autoencoder
run_name: "remucsv2-training"                     # Name of the training run
val_steps: 512                                    # Number of steps between validations
warmup_steps: 500                                 # Number of warmup steps for the learning rate scheduler
validate_at_step_1: True                          # Whether to validate at 1 mod n or 0 mod n