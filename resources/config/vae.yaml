# Config file for autoencoder training

## Dataset parameters
separator: "12_SP-UVR-3B-44100.pth"               # Separator model to use for training.
nstems: 2                                         # Number of stems in the dataset. Has to be adjusted according to the dataset.
dataset_dir: "E:/audio"                           # Directory containing the dataset
output_dir: "E:/output"                           # Directory to save the trained models and checkpoints
val_count: 100                                    # Number of validation samples
sample_rate: 44100                                # Sample rate of the audio
ntimeframes: 512                                  # Number of time frames in the spectrogram
nfft: 1025                                        # Number of features in the spectrogram. Relation with the usual nfft is this = (nfft / 2) + 1

## Model parameters
z_channels: 4                                     # Number of channels in the latent space
codebook_size: 4096                               # Size of each codebook for VQ-VAE
nquantizers: 4                                    # Number of quantizers in the RVQ component
down_channels : [32, 64, 128, 128]                # Number of channels in each downsampling layer
mid_channels : [128, 128]                         # Number of channels in each middle layer
down_sample : [2, 2, 4]                           # Downsampling factors for each downsampling layer
attn_down : [False, False, False]                 # Whether to use attention in each downsampling layer
norm_channels: 32                                 # Number of channels for normalization
num_heads: 4                                      # Number of attention heads
num_down_layers : 2                               # Number of downsampling layers
num_mid_layers : 2                                # Number of middle layers
num_up_layers : 2                                 # Number of upsampling layers

## Training parameters
seed : 1943                                       # Random seed
gradient_checkpointing : True                     # Whether to use gradient checkpointing
num_workers_dl: 0                                 # Number of workers for data loading
batch_size: 2                                     # Batch size for autoencoder training
codebook_weight: 1                                # Weight of codebook loss
commitment_beta: 0.2                              # Weight of commitment loss
entropy_weight: 0.5                               # Weight of entropy loss
steps: 100000                                     # Number of training steps to perform in total
autoencoder_lr: 0.00001                           # Learning rate for the autoencoder
autoencoder_acc_steps: 16                         # Number of accumulation steps for the autoencoder
save_steps: 2048                                  # Number of steps between saving images
ckpt_name: 'vqvae_autoencoder_ckpt.pth'           # Checkpoint name for the VQ-VAE autoencoder
run_name: "remucsv2-training"                     # Name of the training run
val_steps: 512                                    # Number of steps between validations
warmup_steps: 500                                 # Number of warmup steps for the learning rate scheduler
validate_at_step_1: True                          # Whether to validate at 1 mod n or 0 mod n